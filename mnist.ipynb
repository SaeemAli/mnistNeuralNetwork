{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name = 'mnist', with_info = True, as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64) # Make sure it is an integer\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "def scale(image, label): # Scale the pixel values to be between 0 and 1\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255. \n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "scaled_test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Buffer_size = 10000 # Some datasets may be too big to shuffle all at once. Buffer size tells the computer to take 10000 at a time\n",
    "# and only shuffle those, before shuffling the next 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(Buffer_size)\n",
    "\n",
    "# Split data into training and validation\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples) # Take first 10% of the training data for validation\n",
    "train_data = shuffled_train_and_validation_data.skip(num_test_samples) # Take the remaining data for training\n",
    "\n",
    "Batch_size = 100 # Train on 100 samples at a time before adjusting the weights for the next batch\n",
    "train_data = train_data.batch(Batch_size)\n",
    "validation_data = validation_data.batch(num_validation_samples) # Only batched for syntax purpose so we take the whole set as a single batch\n",
    "test_data = scaled_test_data.batch(num_test_samples) # Also doesn't need to be batched\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 # 28 x 28 images. 1 input per pixel\n",
    "output_size = 10 # 10 possible results, 0 to 9\n",
    "hidden_layer_size = 200 # We are using 50 results in each hidden layer. All hidden layers are the same size\n",
    "\n",
    "# Flatten transforms the tensor into a vector\n",
    "# Dense finds the dot products of the inputs and weights and adds the bias, and applies the activation function. This is done between each layer\n",
    "# from the inputs, through the hidden layers up to the output\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation = 'softmax')\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 - 7s - loss: 0.2892 - accuracy: 0.9163 - val_loss: 0.1366 - val_accuracy: 0.9590 - 7s/epoch - 14ms/step\n",
      "Epoch 2/5\n",
      "500/500 - 5s - loss: 0.1081 - accuracy: 0.9676 - val_loss: 0.0990 - val_accuracy: 0.9712 - 5s/epoch - 10ms/step\n",
      "Epoch 3/5\n",
      "500/500 - 5s - loss: 0.0723 - accuracy: 0.9776 - val_loss: 0.0748 - val_accuracy: 0.9773 - 5s/epoch - 10ms/step\n",
      "Epoch 4/5\n",
      "500/500 - 5s - loss: 0.0522 - accuracy: 0.9834 - val_loss: 0.0711 - val_accuracy: 0.9797 - 5s/epoch - 10ms/step\n",
      "Epoch 5/5\n",
      "500/500 - 6s - loss: 0.0381 - accuracy: 0.9878 - val_loss: 0.0663 - val_accuracy: 0.9815 - 6s/epoch - 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2fc312fc100>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 5 # Weights and biases will be adjusted after each epoch\n",
    "\n",
    "model.fit(train_data, epochs = num_epochs, validation_data = (validation_inputs, validation_targets), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 23s 23s/step - loss: 0.0803 - accuracy: 0.9756\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 97.56%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {0:.2f}. Test accuracy: {1:.2f}%\".format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
